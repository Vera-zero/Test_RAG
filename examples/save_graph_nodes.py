from __future__ import annotations

import os, sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import os, json, time, asyncio, logging, re
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any
import datetime

import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, models
from openai import AsyncOpenAI
import aiohttp.client_exceptions
import torch, gc

from graphrag import GraphRAG, QueryParam
from graphrag.base import BaseKVStorage
from graphrag._utils import compute_args_hash, logger, wrap_embedding_func_with_attrs

from graphrag import GraphRAG, QueryParam
import json
from tqdm import tqdm
from pathlib import Path

# --- Local BGE embedding + remote LLM wrapper support ---
class LocalBGEEmbedding:
    def __init__(self, model_path: str, max_token_size: int = 8192):
        self.model_path = model_path
        self.max_token_size = max_token_size
        # lazy init of SentenceTransformer model
        self._model = None

    def _load(self):
        if self._model is not None:
            return
        gpu_count = 0
        try:
            import torch
            gpu_count = torch.cuda.device_count()
            device = "cuda" if gpu_count > 0 else "cpu"
            model_kwargs = {}
            if gpu_count > 1:
                model_kwargs = {"device_map": "auto", "torch_dtype": torch.float16}
        except Exception:
            device = "cpu"
            model_kwargs = {}

        # é¦–å…ˆå°è¯•æŒ‰å¸¸è§„æ–¹å¼åŠ è½½ï¼ˆå½“æ¨¡å‹ç›®å½•ä¸º SentenceTransformers æ ¼å¼æ—¶ï¼‰
        try:
            self._model = SentenceTransformer(
                self.model_path,
                device=device,
                trust_remote_code=True,
                model_kwargs=model_kwargs,
            )
            return
        except Exception:
            # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼ˆä¾‹å¦‚æœ¬åœ°æ¨¡å‹ä¸æ˜¯ä¸¥æ ¼çš„ SentenceTransformer åŒ…è£…ï¼‰ï¼Œ
            # é‚£ä¹ˆå°è¯•æŒ‰æ¨¡å—ç»„åˆçš„æ–¹å¼æ„é€ ï¼šTransformer + Pooling
            try:
                transformer = models.Transformer(self.model_path, max_seq_length=self.max_token_size)
                # è·å– transformer çš„è¾“å‡ºç»´åº¦ä»¥åˆå§‹åŒ– Pooling
                word_emb_dim = transformer.get_word_embedding_dimension()
                pooling = models.Pooling(word_emb_dim,
                                         pooling_mode_mean_tokens=True,
                                         pooling_mode_cls_token=False,
                                         pooling_mode_max_tokens=False)
                self._model = SentenceTransformer(modules=[transformer, pooling])
                # æŠŠæ¨¡å‹ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆSentenceTransformer å†…éƒ¨ä¼šå¤„ç†å¤§éƒ¨åˆ†ï¼Œä½†æ˜¾å¼è®¾ç½®å¯æé«˜å…¼å®¹ï¼‰
                try:
                    import torch
                    if device.startswith("cuda") and torch.cuda.is_available():
                        self._model.to(device)
                except Exception:
                    pass
                return
            except Exception as e:
                # æœ€åå›é€€åˆ°æŠ›å‡ºåŸå§‹å¼‚å¸¸ï¼Œæ–¹ä¾¿æ’æŸ¥
                raise RuntimeError(f"Failed to load embedding model from {self.model_path}: {e}")

    @property
    def embedding_dim(self):
        self._load()
        return self._model.get_sentence_embedding_dimension()

    async def __call__(self, texts):
        # ensure model loaded
        self._load()
        if isinstance(texts, str):
            texts = [texts]
        loop = asyncio.get_event_loop()
        fn = lambda: self._model.encode(
            texts,
            batch_size=32,
            convert_to_numpy=True,
            show_progress_bar=False,
            normalize_embeddings=True,
        )
        if loop.is_running():
            return await loop.run_in_executor(None, fn)
        else:
            return fn()


# Check for OPENAI_API_KEY environment variable
def check_deepseek_api_key():
    """
    Check if OPENAI_API_KEY is set and not empty.
    If not set, allow user to input manually.
    Raises SystemExit if not properly configured.
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    
    if api_key is None or not api_key.strip():
        print("âŒ DEEPSEEK_API_KEY environment variable is not set or empty.")
        print("\nOptions:")
        print("1. Set environment variable: export DEEPSEEK_API_KEY='your-api-key-here'")
        print("2. Enter API key manually now (will be set for this session)")
        
        choice = input("\nWould you like to enter your API key manually? (y/N): ")
        if choice.lower() == 'y':
            manual_key = input("Please enter your DeepSeek API key: ").strip()
            if not manual_key:
                print("âŒ Error: No API key provided.")
                sys.exit(1)
            # Set the environment variable for this session (set both for compatibility)
            os.environ["DEEPSEEK_API_KEY"] = manual_key
            os.environ["OPENAI_API_KEY"] = manual_key
            api_key = manual_key
            print("âœ… DEEPSEEK_API_KEY has been set for this session.")
        else:
            print("âŒ Cannot proceed without API key.")
            sys.exit(1)
    
    # Basic format validation (OpenAI keys typically start with 'sk-')
    if not api_key.startswith('sk-'):
        print("âš ï¸  Warning: DEEPSEEK_API_KEY doesn't appear to be in the expected format (should start with 'sk-')")
        print(f"   Current key starts with: {api_key[:10]}...")
        response = input("Do you want to continue anyway? (y/N): ")
        if response.lower() != 'y':
            sys.exit(1)
    
    print("âœ… DEEPSEEK_API_KEY is properly configured.")
    return True

def read_json_file(fp: Path):
    """è¯»å– JSON æ–‡ä»¶"""
    with fp.open(encoding="utf-8") as f:
        return json.load(f)

async def save_graph_nodes(graph_func: GraphRAG, output_dir: Path):
    """
    ä¿å­˜å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹åˆ° JSON æ–‡ä»¶
    
    Args:
        graph_func: GraphRAG å®ä¾‹
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir.mkdir(exist_ok=True)
        
        # è·å–äº‹ä»¶å›¾å­˜å‚¨å®ä¾‹
        event_graph = graph_func.event_dynamic_graph
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        all_nodes = await event_graph.get_all_nodes()
        
        if not all_nodes:
            logger.warning("å›¾ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹")
            return
        
        # å‡†å¤‡èŠ‚ç‚¹æ•°æ®
        nodes_data = {
            "metadata": {
                "export_time": datetime.datetime.now().isoformat(),
                "total_nodes": len(all_nodes),
                "graph_type": "dynamic_event_graph"
            },
            "nodes": {}
        }
        
        # å¤„ç†æ¯ä¸ªèŠ‚ç‚¹
        for node_id, node_data in all_nodes.items():
            if node_data:
                # æ¸…ç†èŠ‚ç‚¹æ•°æ®ï¼Œç¡®ä¿å¯åºåˆ—åŒ–
                clean_node_data = {}
                for key, value in node_data.items():
                    if isinstance(value, (str, int, float, bool, list, dict, type(None))):
                        clean_node_data[key] = value
                
                nodes_data["nodes"][node_id] = clean_node_data
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"graph_nodes_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(nodes_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(nodes_data['nodes'])} ä¸ªèŠ‚ç‚¹åˆ°: {output_file}")
        
        # æ‰“å°èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»èŠ‚ç‚¹æ•°: {len(nodes_data['nodes'])}")
        
        # æŒ‰èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
        node_types = {}
        for node_id, node_data in nodes_data['nodes'].items():
            node_type = node_data.get('entity_type', 'unknown')
            node_types[node_type] = node_types.get(node_type, 0) + 1
        
        print(f"   - èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        for node_type, count in node_types.items():
            print(f"     * {node_type}: {count} ä¸ª")
        
        return nodes_data
        
    except Exception as e:
        logger.error(f"ä¿å­˜èŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
        raise

async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ API å¯†é’¥
    check_deepseek_api_key()
    
    # è®¾ç½®å·¥ä½œç›®å½•
    WORK_DIR = Path("work_dir")
    WORK_DIR.mkdir(exist_ok=True)
    
    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = Path("saved_nodes")
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    CORPUS_FILE = Path("../demo/Corpus.json")
    
    logging.basicConfig(level=logging.INFO)
    logging.getLogger("DyG-RAG").setLevel(logging.INFO)
    
    # ä½¿ç”¨æœ¬åœ° BGE åµŒå…¥
    local_bge_path = os.getenv("LOCAL_BGE_PATH", os.path.join(os.path.dirname(__file__), "..", "models", "bge_m3"))
    embedding_func = LocalBGEEmbedding(local_bge_path)
    
    # ç¡®ä¿æ¨¡å‹åŠ è½½å¹¶åŒ…è£…åµŒå…¥å‡½æ•°
    try:
        embedding_func._load()
        emb_dim = embedding_func.embedding_dim
    except Exception:
        emb_dim = getattr(embedding_func, 'embedding_dim', 1536)
    
    embedding_func = wrap_embedding_func_with_attrs(embedding_dim=emb_dim, max_token_size=embedding_func.max_token_size)(embedding_func)
    
    # åˆ›å»º GraphRAG å®ä¾‹
    graph_func = GraphRAG(
        working_dir=str(WORK_DIR),
        embedding_func=embedding_func,
        best_model_max_token_size=16384,
        cheap_model_max_token_size=16384
    )
    
    # è¯»å–è¯­æ–™æ–‡ä»¶
    if not CORPUS_FILE.exists():
        logger.error(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨: {CORPUS_FILE}")
        return
    
    corpus_data = read_json_file(CORPUS_FILE)
    total_docs = len(corpus_data)
    logger.info(f"å¼€å§‹å¤„ç†ï¼Œå…±æœ‰ {total_docs} ä¸ªæ–‡æ¡£")
    
    # å‡†å¤‡æ–‡æ¡£
    all_docs = []
    for idx, obj in enumerate(tqdm(corpus_data, desc="åŠ è½½æ–‡æ¡£", total=total_docs)):
        enriched_content = f"Title: {obj['title']}\nDocument ID: {obj['doc_id']}\n\n{obj['context']}"
        all_docs.append(enriched_content)
    
    # æ’å…¥æ–‡æ¡£å¹¶æ„å»ºå›¾
    logger.info("å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
    # ä¿®å¤ï¼šä½¿ç”¨å¼‚æ­¥æ’å…¥æ–¹æ³•
    await graph_func.ainsert(all_docs)
    logger.info("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
    
    # ä¿å­˜èŠ‚ç‚¹
    logger.info("å¼€å§‹ä¿å­˜èŠ‚ç‚¹æ•°æ®...")
    nodes_data = await save_graph_nodes(graph_func, OUTPUT_DIR)
    
    # å¯é€‰ï¼šæ‰§è¡Œä¸€ä¸ªæŸ¥è¯¢æ¥éªŒè¯å›¾çš„åŠŸèƒ½
    logger.info("æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢...")
    try:
        # ä¿®å¤ï¼šä½¿ç”¨å¼‚æ­¥æŸ¥è¯¢æ–¹æ³•
        result = await graph_func.aquery("Which position did Pat Duncan hold in Feb 1996?", param=QueryParam(mode="dynamic"))
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ç»“æœ:")
        print(result)
    except Exception as e:
        logger.warning(f"æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}")
    
    logger.info("è„šæœ¬æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())